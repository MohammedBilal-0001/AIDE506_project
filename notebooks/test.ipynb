{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Let's assume the dataset is already preprocessed as mentioned\n",
    "# Load the dataset\n",
    "def load_data(file_path='telco_customer_churn.csv'):\n",
    "    \"\"\"\n",
    "    Load the preprocessed telco customer churn dataset\n",
    "    \"\"\"\n",
    "    # For demonstration, we'll create a synthetic dataset if file not provided\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    except:\n",
    "        print(\"Creating synthetic dataset for demonstration\")\n",
    "        # Create synthetic data matching the description (7043 customers, 20 features)\n",
    "        np.random.seed(42)\n",
    "        n_samples = 7043\n",
    "        \n",
    "        # Create features similar to telecom churn datasets\n",
    "        df = pd.DataFrame({\n",
    "            'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "            'SeniorCitizen': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "            'Partner': np.random.choice(['Yes', 'No'], n_samples),\n",
    "            'Dependents': np.random.choice(['Yes', 'No'], n_samples),\n",
    "            'tenure': np.random.randint(0, 73, n_samples),\n",
    "            'PhoneService': np.random.choice(['Yes', 'No'], n_samples, p=[0.9, 0.1]),\n",
    "            'MultipleLines': np.random.choice(['Yes', 'No', 'No phone service'], n_samples),\n",
    "            'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples),\n",
    "            'OnlineSecurity': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'OnlineBackup': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'DeviceProtection': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'TechSupport': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'StreamingTV': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'StreamingMovies': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "            'Contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.55, 0.25, 0.2]),\n",
    "            'PaperlessBilling': np.random.choice(['Yes', 'No'], n_samples),\n",
    "            'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
    "            'MonthlyCharges': np.random.uniform(20, 120, n_samples),\n",
    "            'TotalCharges': np.random.uniform(100, 8000, n_samples),\n",
    "        })\n",
    "        \n",
    "        # Generate target with realistic class imbalance (around 25% churn rate)\n",
    "        churn_prob = 0.2 + 0.4 * (df['Contract'] == 'Month-to-month') - 0.2 * (df['tenure'] > 40)\n",
    "        df['Churn'] = np.random.binomial(1, churn_prob, n_samples)\n",
    "        df['Churn'] = df['Churn'].map({1: 'Yes', 0: 'No'})\n",
    "        \n",
    "    # Check for class imbalance\n",
    "    print(f\"Class distribution: {df['Churn'].value_counts(normalize=True).round(3) * 100}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data for modeling\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Target variable conversion\n",
    "    if data['Churn'].dtypes == 'object':\n",
    "        data['Churn'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Convert categorical variables to dummy variables\n",
    "    # First, identify categorical columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    categorical_cols = [col for col in categorical_cols if col != 'Churn']\n",
    "    \n",
    "    # Create dummy variables\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Split features and target\n",
    "    X = data.drop('Churn', axis=1)\n",
    "    y = data['Churn']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Handle class imbalance using SMOTE (only on training data)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Original training set shape: {y_train.value_counts()}\")\n",
    "    print(f\"Balanced training set shape: {pd.Series(y_train_balanced).value_counts()}\")\n",
    "    \n",
    "    # Scale the numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    X_train_balanced[numerical_cols] = scaler.fit_transform(X_train_balanced[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test, list(X.columns)\n",
    "\n",
    "def build_stacked_ensemble():\n",
    "    \"\"\"\n",
    "    Build a stacked ensemble of diverse classifiers\n",
    "    \"\"\"\n",
    "    # Define base classifiers\n",
    "    base_classifiers = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Meta-classifier\n",
    "    meta_clf = LogisticRegression(random_state=42)\n",
    "    \n",
    "    return base_classifiers, meta_clf\n",
    "\n",
    "def train_stacked_ensemble(X_train, y_train, base_classifiers, meta_clf):\n",
    "    \"\"\"\n",
    "    Train the stacked ensemble model\n",
    "    \"\"\"\n",
    "    # Train base classifiers\n",
    "    base_models = []\n",
    "    X_meta = np.zeros((X_train.shape[0], len(base_classifiers)))\n",
    "    \n",
    "    # Cross-validation for creating meta features\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for i, (name, clf) in enumerate(base_classifiers):\n",
    "        base_models.append(clf.fit(X_train, y_train))\n",
    "        \n",
    "        # Use cross-validation to create meta-features\n",
    "        cv_preds = np.zeros(X_train.shape[0])\n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            # Train the classifier on the training fold\n",
    "            clf_cv = clf.__class__(**clf.get_params())\n",
    "            clf_cv.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "            \n",
    "            # Predict on validation fold\n",
    "            cv_preds[val_idx] = clf_cv.predict_proba(X_train.iloc[val_idx])[:, 1]\n",
    "            \n",
    "        X_meta[:, i] = cv_preds\n",
    "    \n",
    "    # Train meta-classifier on meta-features\n",
    "    meta_clf.fit(X_meta, y_train)\n",
    "    \n",
    "    return base_models, meta_clf\n",
    "\n",
    "def predict_with_ensemble(X, base_models, meta_clf, base_classifiers):\n",
    "    \"\"\"\n",
    "    Generate predictions using the stacked ensemble\n",
    "    \"\"\"\n",
    "    # Generate meta-features using base models\n",
    "    X_meta = np.zeros((X.shape[0], len(base_classifiers)))\n",
    "    \n",
    "    for i, (_, _) in enumerate(base_classifiers):\n",
    "        X_meta[:, i] = base_models[i].predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Predict using meta-classifier\n",
    "    y_pred_proba = meta_clf.predict_proba(X_meta)[:, 1]\n",
    "    y_pred = meta_clf.predict(X_meta)\n",
    "    \n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance\n",
    "    \"\"\"\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def explain_with_shap(X_train, X_test, base_models):\n",
    "    \"\"\"\n",
    "    Generate SHAP explanations for model predictions\n",
    "    \"\"\"\n",
    "    # We'll use the RandomForest model for SHAP explanations as it's more interpretable\n",
    "    rf_model = base_models[0]  # Assuming RandomForest is the first model\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    \n",
    "    # Compute SHAP values for test data (sample for visualization)\n",
    "    sample_size = min(100, X_test.shape[0])\n",
    "    X_sample = X_test.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title('Feature Importance based on SHAP Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed SHAP visualization for top 3 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values[1], X_sample, show=False)\n",
    "    plt.title('SHAP Value Distribution by Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top features based on SHAP values\n",
    "    feature_importance = np.abs(shap_values[1]).mean(0)\n",
    "    feature_names = X_test.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df, explainer, shap_values\n",
    "\n",
    "def identify_high_risk_customers(X_test, y_pred_proba, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Identify high-risk customers and recommend retention strategies\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with customer features and churn probability\n",
    "    high_risk_df = X_test.copy()\n",
    "    high_risk_df['Churn_Probability'] = y_pred_proba\n",
    "    \n",
    "    # Identify high-risk customers (high churn probability)\n",
    "    high_risk_customers = high_risk_df[high_risk_df['Churn_Probability'] > threshold]\n",
    "    \n",
    "    print(f\"\\nIdentified {len(high_risk_customers)} high-risk customers (churn probability > {threshold})\")\n",
    "    \n",
    "    return high_risk_customers\n",
    "\n",
    "def develop_retention_strategies(importance_df, high_risk_df, feature_names):\n",
    "    \"\"\"\n",
    "    Develop personalized retention strategies based on SHAP explanations\n",
    "    \"\"\"\n",
    "    # Get top 5 influential features\n",
    "    top_features = importance_df.head(5)['Feature'].values\n",
    "    \n",
    "    # Define strategy templates based on feature categories\n",
    "    strategies = {\n",
    "        # Contract-related\n",
    "        'Contract_Month-to-month': \"Offer discounted 1-year or 2-year contracts with incentives\",\n",
    "        'Contract_One year': \"Provide early renewal bonuses for upgrading to 2-year contract\",\n",
    "        'Contract_Two year': \"Reward loyalty with premium service upgrades\",\n",
    "        \n",
    "        # Services\n",
    "        'InternetService_Fiber optic': \"Address potential service quality issues, offer network improvements\",\n",
    "        'InternetService_DSL': \"Offer upgrade to fiber with first 3 months at DSL price\",\n",
    "        'OnlineSecurity_No': \"Provide free trial of online security services\",\n",
    "        'TechSupport_No': \"Offer discounted tech support services\",\n",
    "        'OnlineBackup_No': \"Provide free cloud backup service for 3 months\",\n",
    "        'DeviceProtection_No': \"Bundle device protection with other services at discount\",\n",
    "        \n",
    "        # Billing\n",
    "        'PaperlessBilling_Yes': \"Offer discount for annual prepayment\",\n",
    "        'PaymentMethod_Electronic check': \"Provide discount for switching to automatic credit card payments\",\n",
    "        'MonthlyCharges': \"Offer personalized discount based on usage patterns\",\n",
    "        'TotalCharges': \"Provide loyalty discount based on tenure and total spend\",\n",
    "        \n",
    "        # Demographics\n",
    "        'SeniorCitizen': \"Offer senior-specific service bundles with simplified options\",\n",
    "        'tenure': \"Recognize customer loyalty with special anniversary offers\",\n",
    "        'Partner_Yes': \"Offer family plan upgrades\",\n",
    "        'Dependents_Yes': \"Provide family-friendly content packages and parental controls\",\n",
    "    }\n",
    "    \n",
    "    # Create general strategies based on top influential features\n",
    "    print(\"\\nGeneral Retention Strategies Based on Top Influential Features:\")\n",
    "    for feature in top_features:\n",
    "        base_feature = feature.split('_')[0] if '_' in feature else feature\n",
    "        if feature in strategies:\n",
    "            print(f\"- For {feature}: {strategies[feature]}\")\n",
    "        elif base_feature in strategies:\n",
    "            print(f\"- For {feature}: {strategies[base_feature]}\")\n",
    "    \n",
    "    # Sample a few high-risk customers for personalized strategies\n",
    "    sample_size = min(5, len(high_risk_df))\n",
    "    if sample_size > 0:\n",
    "        sampled_customers = high_risk_df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        print(\"\\nPersonalized Retention Strategies for Sample High-Risk Customers:\")\n",
    "        for i, (_, customer) in enumerate(sampled_customers.iterrows()):\n",
    "            print(f\"\\nCustomer {i+1} (Churn Probability: {customer['Churn_Probability']:.2f}):\")\n",
    "            \n",
    "            # Identify top 3 features for this customer\n",
    "            # This is a simplified approach - in a real implementation we would use\n",
    "            # individual SHAP values for each prediction\n",
    "            customer_features = []\n",
    "            for feature in feature_names:\n",
    "                if feature in customer.index and customer[feature] > 0:\n",
    "                    if feature in importance_df['Feature'].values:\n",
    "                        importance = importance_df[importance_df['Feature'] == feature]['Importance'].values[0]\n",
    "                        customer_features.append((feature, importance, customer[feature]))\n",
    "            \n",
    "            # Sort by importance\n",
    "            customer_features.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Recommend strategies based on top 3 features\n",
    "            for j, (feature, _, value) in enumerate(customer_features[:3]):\n",
    "                base_feature = feature.split('_')[0] if '_' in feature else feature\n",
    "                if feature in strategies:\n",
    "                    print(f\"  {j+1}. {strategies[feature]}\")\n",
    "                elif base_feature in strategies:\n",
    "                    print(f\"  {j+1}. {strategies[base_feature]}\")\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_data(\"G:\\LebanseUni\\M2\\S1\\AIDE506-Advanced Machine learining Topics\\Project\\Data\\processed_data.csv\")\n",
    "    X_train, X_test, y_train, y_test, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Build and train stacked ensemble\n",
    "    print(\"\\nBuilding and training stacked ensemble model...\")\n",
    "    base_classifiers, meta_clf = build_stacked_ensemble()\n",
    "    base_models, meta_clf = train_stacked_ensemble(X_train, y_train, base_classifiers, meta_clf)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    y_pred, y_pred_proba = predict_with_ensemble(X_test, base_models, meta_clf, base_classifiers)\n",
    "    metrics = evaluate_model(y_test, y_pred, y_pred_proba)\n",
    "    \n",
    "    # Generate SHAP explanations\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    importance_df, explainer, shap_values = explain_with_shap(X_train, X_test, base_models)\n",
    "    print(\"\\nTop 10 features by importance:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # Identify high-risk customers\n",
    "    high_risk_customers = identify_high_risk_customers(X_test, y_pred_proba)\n",
    "    \n",
    "    # Develop retention strategies\n",
    "    develop_retention_strategies(importance_df, high_risk_customers, feature_names)\n",
    "    \n",
    "    print(\"\\nModel building, evaluation, and actionable insights generation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
